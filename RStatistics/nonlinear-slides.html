<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Nonlinear Regression with R</title>
    <meta charset="utf-8" />
    <meta name="author" content="Thomas Petzoldt" />
    <meta name="date" content="2022-11-22" />
    <script src="libs/header-attrs-2.18.1/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/useR-fonts.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-webcam-0.0.1/webcam.js"></script>
    <script id="xaringanExtra-webcam-options" type="application/json">{"width":"200","height":"200","margin":"1em"}</script>
    <link href="libs/tile-view-0.2.6/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view-0.2.6/tile-view.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="tp_xaringan_scrollable.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">



class: center, middle, title-slide

background-image: url("img/water_tud_r_oer_by.png")

background-size: cover


&lt;!-- Own title slide / --&gt;

# A small intro to nonlinear regression with **R**

#### Thomas Petzoldt

.small[Version 2022-11-22, Source code available on https://github.com/tpetzoldt]


--- 

Use cursor keys for navigation, press .red["O"] for a slide .red[O]verview

&lt;!--
Verbatim code embedding:
https://themockup.blog/posts/2021-08-27-displaying-verbatim-code-chunks-in-xaringan-presentations/
--&gt;

&lt;!--- Setup -------------------------------------------------------------------&gt;






&lt;!-- citations work differently with xaringan compared to @Markdown / --&gt;

&lt;!--- End of Setup ------------------------------------------------------------&gt;


&lt;style&gt;.bigfont {
  font-size: 140%;
  line-height: 160%;
}&lt;/style&gt;

&lt;style&gt;.bigcode {
  font-size: 140%;
  line-height: 140%;
}&lt;/style&gt;


---

## Nonlinear regression

Many phenomena in nature are non-linear!

**Linear or non-linear?**

* Some non-linear problems can be solved with linear methods
* e.g., polynomials or periodic (sine and cosine) functions
* others can be fitted by using transformations

**Example**

`$$y = a \cdot x^b$$`
can be transformed to:
`$$\ln(y) = \ln(a) + b \cdot \ln(x)$$`

* but: linearization transforms also the residuals
* transformation is sometimes correct and sometimes wrong
* homogeneity of variances

---

## Linearization or direct nonlinear fitting?

### Linearising transformations

* log, square root, reciprocals ...
* can be applied if the residual variance remains (or is becoming) homogenous.
* but in some cases: transformations lead to heteroscedasticity&lt;br&gt;
  `\(\Rightarrow\)` **biased** regression parameters.

### Nonlinear regression

* very flexible, user-defined functions,
* no transformation required
* but: requires **iterative** optimization methods,
* in theory only local optima can be found,
* parameters are not in all cases **identifiable**.

---

## Nonlinear regression
### Iterative search for the minimum of the sum of squares

![:scale 68%](img/optimization.png)
---
## How to find the global minimum?

In all cases where an analytical solution of the partial derivatives
is unknown or not existent or if linearization leads to a
violation of the regression assumptions, a numerical optimization
method has to be used.

#### Goodness of fit

The quality of the fit can be measured by the sum of least squares `\(SQ\)`:

$$
SQ = \sum_{i=1}^n\left(y_i- f(\mathbf x_i, \mathbf p)\right)^2 = \min !
$$

with `\(y\)`: dependent variable, `\(\mathbf x\)`: matrix of independent variables,
`\(\mathbf p\)`: parameter vector, `\(n\)`: sample size.

The nonlinear coefficient of determination `\(R^2\)` can be calculated from the
variance of the residuals `\(\varepsilon\)`, compared to the variance of the original data `\(y\)`:

$$
R^2 = 1 - \frac{\text{variance of the residuals}}{\text{variance of the y-data}} = 1- \frac{s^2_\varepsilon}{s^2_y}
$$
---

## General principle of algorithms

.bigfont[
The minimum of the squared residuals (SQ) is searched by iteration:

1. Start from an initial guess for the parameter set.
2. Try to find a parameter set with lower SQ.
3. Is the new parameter set better?
    * No: Reject the new parameters and goto 2
    * Yes: Accept the new parameters and goto 4
4. Is the new parameter set good enough?
    * No: goto 2
    * Yes: goto 5
5. Parameter set found.
]
---
## Deterministic Methods

### Gradient Search

* go one step into the direction of steepest descent
* `\(\rightarrow\)` relatively simple
* `\(\rightarrow\)` relatively robust for "more complicated'' problems



### Newton's Method

* quadratic approximation of the SQ function
* try to estimate the minimum
* `\(\rightarrow\)` takes curvature into account
* `\(\rightarrow\)` faster for "well behaving" problems
* several versions: quasi-Newton, Gauss-Newton, L-BFGS, ...




**Levenberg-Marquardt** interpolates between Gauss-Newton and gradient descent.

---

## Newton vs. Gradient Search

![:scale 30%](img/newton_vs_grad_descent.png)

Newton method (red) uses curvature information to converge faster than gradient descent (green).

See also https://en.wikipedia.org/wiki/Newton's_method_in_optimization


---

## Stochastic Methods

Use statistical principles (random search)

**Classical methods**

* Simulated annealing: simulates heating and cooling of matter `\(\rightarrow\)` "Crystallisation process".
* Controlled random search  &lt;a name=cite-Price1977&gt;&lt;/a&gt;&lt;a name=cite-Price1983&gt;&lt;/a&gt;([Price, 1977](#bib-Price1977); [Price, 1983](#bib-Price1983))


**Evolutionary Algorithms**

* analogy to genetics: mutation and selection
* follows a "population" of several parameter sets in parallel
* information exchange ("crossover") between parameter sets
* `\(\rightarrow\)` for complicated problems with large number of parameters
* nowadays builtin in Microsoft Excel and LibreOffice Calc

... and many more.

---
## Examples

.bigfont[
* Enzyme kinetics
* Growth of organisms
* Calibration of complex models in chemistry, hydrology, hydrophysics, groundwater, wastewater, water quality ... finance, business and social sciences.
]

---
## Enzyme Kinetics

... can be described with the well-known Michaelis-Menten function:

![](nonlinear-slides_files/figure-html/enzymekinetics-1.png)&lt;!-- --&gt;

---

## Linearization vs. (true) nonlinear regression

**Linearizing transformation**

[&gt;] Appropriate if transformation improves homogeneity of variances &lt;br&gt;
[+] Fast, simple and easy.&lt;br&gt;
[+] Analytical solution returns the global optimum.&lt;br&gt;
[-] Only a limited set of functions can be fitted.&lt;br&gt;
[-] Can lead to wrongly transformed error structure and biased results.



**Nonlinear Regression**

[&gt;] Appropriate if error structure is already homogeneous and/or analytical solution does not exist.&lt;br&gt;
[+] Can be used to fit arbitrary functions, given that the parameters are identifiable.&lt;br&gt;
[-] Needs start values and considerable computation time.&lt;br&gt;
[-] Best solution (global optimum) is not guaranteed.


---
## Nonlinear regression: simple exponential
.bigcode[

```r
x &lt;- 1:10
y &lt;- c(1.6, 1.8, 2.1, 2.8, 3.5, 4.1, 5.1, 5.8, 7.1, 9.0)
plot(x, y)
pstart &lt;- c(a = 1, b = 1)
aFit &lt;- nls(y ~ a * exp(b * x), start = pstart)

x1 &lt;- seq(1, 10, 0.1)
y1 &lt;- predict(aFit, data.frame(x = x1))
lines(x1, y1, col = "red")

summary(aFit)
```
]

![](nonlinear-slides_files/figure-html/nlregexp-1.png)&lt;!-- --&gt;

```
## 
## Formula: y ~ a * exp(b * x)
## 
## Parameters:
##   Estimate Std. Error t value Pr(&gt;|t|)    
## a 1.263586   0.049902   25.32 6.34e-09 ***
## b 0.194659   0.004716   41.27 1.31e-10 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.1525 on 8 degrees of freedom
## 
## Number of iterations to convergence: 13 
## Achieved convergence tolerance: 2.504e-08
```


```r
(Rsquared &lt;- 1 - var(residuals(aFit))/var(y))
```

```
## [1] 0.9965644
```

---
## Michaelis-Menten-Kinetics

### Code

.bigcode[

```r
S &lt;-c(25, 25, 10, 10, 5, 5, 2.5, 2.5, 1.25, 1.25)
V &lt;-c(0.0998, 0.0948, 0.076, 0.0724, 0.0557,
      0.0575, 0.0399, 0.0381, 0.017, 0.0253)

## Michaelis-Menten kinetics
f &lt;- function(S, Vm, K) { Vm * S/(K + S) }

pstart &lt;- c(Vm = 0.1, K = 5)
aFit &lt;- nls(V ~ f(S, Vm, K), start = pstart)
summary(aFit)

(1 - var(residuals(aFit))/var(V)) # nonlinear rÂ²
plot(S, V, xlim = c(0, max(S)), ylim = c(0, max(V)))
x1 &lt;-seq(0, 25, length = 100)
lines(x1, predict(aFit, data.frame(S = x1)), col = "red")
```
]

### Results


```
## 
## Formula: V ~ f(S, Vm, K)
## 
## Parameters:
##    Estimate Std. Error t value Pr(&gt;|t|)    
## Vm  0.11713    0.00381   30.74 1.36e-09 ***
## K   5.38277    0.46780   11.51 2.95e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.003053 on 8 degrees of freedom
## 
## Number of iterations to convergence: 3 
## Achieved convergence tolerance: 6.714e-06
```

```
## [1] 0.9895147
```

![](nonlinear-slides_files/figure-html/mmenten-1.png)&lt;!-- --&gt;


---
## Michaelis-Menten-Kinetics with selfstart

* Function `SSmicmen` determines start parameters automatically.
* Only few selfstart functions available in R

### Code

.bigcode[

```r
S &lt;- c(25, 25, 10, 10, 5, 5, 2.5, 2.5, 1.25, 1.25)
V &lt;- c(0.0998, 0.0948, 0.076, 0.0724, 0.0557,
       0.0575, 0.0399, 0.0381, 0.017, 0.0253)

aFit &lt;- nls(V ~ SSmicmen(S, Vm, K))
summary(aFit)

plot(S, V, xlim = c(0, max(S)), ylim = c(0, max(V)))
x1 &lt;-seq(0, 25, length = 100)
lines(x1, predict(aFit, data.frame(S = x1)), col="red")
```
]

### Results


```
## 
## Formula: V ~ SSmicmen(S, Vm, K)
## 
## Parameters:
##    Estimate Std. Error t value Pr(&gt;|t|)    
## Vm  0.11713    0.00381   30.74 1.36e-09 ***
## K   5.38276    0.46780   11.51 2.95e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.003053 on 8 degrees of freedom
## 
## Number of iterations to convergence: 0 
## Achieved convergence tolerance: 1.214e-07
```

![](nonlinear-slides_files/figure-html/mmselfstart-1.png)&lt;!-- --&gt;

---
### Correlation of parameters


```r
summary(aFit, correlation=TRUE)
```

```
## 
## Formula: V ~ SSmicmen(S, Vm, K)
## 
## Parameters:
##    Estimate Std. Error t value Pr(&gt;|t|)    
## Vm  0.11713    0.00381   30.74 1.36e-09 ***
## K   5.38276    0.46780   11.51 2.95e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.003053 on 8 degrees of freedom
## 
## Correlation of Parameter Estimates:
##   Vm  
## K 0.88
## 
## Number of iterations to convergence: 0 
## Achieved convergence tolerance: 1.214e-07
```

* high absolute values of correlation indicate non-identifiability of parameters
* critical value depends on the data
* sometimes, better start values or another optimization algorithm can help

---
## Practical hints

* plot data
* find good starting values by thinking about it or by trial and error
* avoid very small and/or very large numbers&lt;br&gt;
  `\(\longrightarrow\)` rescale the problem to values between approx 0.001 to 1000
* start with a simple function and add terms and parameters sequentially
* Don't take significance of parameters too seriously. A non-significant parameter 
may be necessary for the structure of the model, removal of it will invalidate the whole model.




---

## Further reading

* Package **growthrates** for growth curves: https://cran.r-project.org/package=growthrates
* Package **FME** for more complex model fitting tasks (identifiability analysis, constrained optimization, multiple dependent variables and MCMC): &lt;a name=cite-FME&gt;&lt;/a&gt;([Soetaert and Petzoldt, 2010](https://doi.org/10.18637/jss.v033.i03)), https://cran.r-project.org/package=FME 

* More about optimization in **R**: https://cran.r-project.org/web/views/Optimization.html



---


## References


&lt;a name=bib-Price1977&gt;&lt;/a&gt;[Price, W. L.](#cite-Price1977) (1977). "A
controlled random search procedure for global optimization". In: _The
Computer Journal_ 20.4, pp. 367-370.

&lt;a name=bib-Price1983&gt;&lt;/a&gt;[Price, W. L.](#cite-Price1983) (1983).
"Global optimization by controlled random search". In: _Journal of
Optimization Theory and Applications_ 40.3, pp. 333-348.

&lt;a name=bib-FME&gt;&lt;/a&gt;[Soetaert, K. and T. Petzoldt](#cite-FME) (2010).
"Inverse Modelling, Sensitivity and Monte Carlo Analysis in R Using
Package FME". In: _Journal of Statistical Software_ 33.3, pp. 1-28.
DOI: [10.18637/jss.v033.i03](https://doi.org/10.18637%2Fjss.v033.i03).

---
## Appendix

---

## Lineweaver-Burk transformation vs. nonlinear fit

See: https://en.wikipedia.org/wiki/Lineweaver-Burk_plot

### Code


```r
S &lt;-c(25, 25, 10, 10, 5, 5, 2.5, 2.5, 1.25, 1.25)
V &lt;-c(0.0998, 0.0948, 0.076, 0.0724, 0.0557, 
      0.0575, 0.0399, 0.0381, 0.017, 0.0253)
aFit&lt;-nls(V ~ SSmicmen(S, Vm, K))

par(mfrow=c(1,2), las=1, lwd=2)
## Lineweaver-Burk
x &lt;- 1/S; y &lt;- 1/V

plot(x, y, xlab="1/S", ylab="1/V", xlim=c(-0.2,1), ylim=c(0, 80), pch=16,
  main="Linearisation\n(Lineweaver-Burk)")
abline(h=0, lwd=1, col="grey")
abline(v=0, lwd=1, col="grey")
m &lt;- lm(y ~ x)
abline(m, col = "red")
Vm_l &lt;- 1/coef(m)[1]
Km_l &lt;- coef(m)[2] * Vm_l
#legend("topleft", c("vmax = 1/intercept", "km = slope * vmax"), 
#        box.lwd=1, bg="white")
text(0.35, 75, "1/V = 1/vmax + km / vmax * S")

## retransformed, both together
plot(S, V, xlim = c(0, max(S)),ylim=c(0, max(V)), pch=16, main="No Transformation")
x1 &lt;-seq(0, 25, length=100)
lines(x1, Vm_l * x1 / (Km_l + x1), col="red")
lines(x1, predict(aFit, data.frame(S=x1)), col="blue")
legend("bottomright", legend=c("linearisation", "nonlinear"), 
       box.lwd=1, lwd=2, col=c("red", "blue"))
text(15, 0.04, "V = S * vmax / (km + S)")
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="tp_xaringan.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"self_contained": true,
"navigation": {
"scroll": false
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
