---
# https://garrettgman.github.io/rmarkdown/slidy_presentation_format.html
title: "One and two-way ANOVA with R"
author: "Thomas Petzoldt"
date: "2021-01-19"
bibliography: bib.bib
output:
  slidy_presentation:
    font_adjustment: 0
    css: tp_slidy.css
    theme: default
    highlight: default
    #df_print: kable
    #mathjax: local
    pandoc_args: -V slidy-url=myslidy
    self_contained: no
---

Preface
--------------------------------------------------------------------------------

* This HTML document is intended to amend the PDF lecture slides.
* The hybrid format of "long slides" aims to make self-study easier. Compared to the
pdf version more code is shown to improve reproducibility. Just copy the code
to your favorite R editor (e.g. RStudio).
* Full source code of the slides is provided in R Markdown format using the "slidy" framework.
* Photos and algae growth data were created as part of a highschool internship project, 
many thanks to C. Belger for his contribution.
* Please send questions and comments to: https://tu-dresden.de/Members/thomas.petzoldt

### Thank you!

<style>
 h2, h3 {
   color: #009de0;
   font-size: 40px;
}

div.vbox {
  float: left;
  height: 40%; width: 50%;
  margin-top: -220px;
}
div.hbox {
  width:60%;  margin-top: 0;
  margin-left:auto; margin-right:auto;
  height: 60%;
  border:1px solid silver;
  // background:#F0F0F0;
  overflow:auto;
  text-align:left;
  clear:both;
}
.math {
  font-size: 95%;
  padding-right: 1ex;
}
li::marker {
  color: orange;
}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library("dplyr")
library("tidyr")
library("kableExtra")
mypar <- list(las=1, cex.lab=1.4, cex.axis=1.4, lwd=2)
```


ANOVA – Analysis of Variances
--------------------------------------------------------------------------------
* Testing of complex hypothesis as a whole, e.g.:
    * more than two samples (multiple test problem),
    * several multiple factors (multiway ANOVA)
    * elimination of covariates (ANCOVA)
    * fixed and/or random effects
    (variance decomposition methods, mixed effects models)
* Different application scenarios:
    * explorative use: Which influence factors are important?
    * descriptive use: Fitting of models for process description and
    forecasting.
    * significance tests.
* ANOVA methods are (in most cases) based on linear models.



A practical example
--------------------------------

![](img/ansaetze.jpg)


### Scientific question

Find a suitable medium for growth experiments with green algae:

* Cheap, easy to handle
* Suitable for students courses and classroom experiments

### Idea


* Use a commercial fertilizer with the main nutrients N and P
* Mineral water with trace elements
* Does non-sparkling mineral water contain enough $\mathrm{HCO_3^-}$?
* Test how to improve ($\mathrm{CO_2}$) availability for photosynthesis

Application
--------------------------------------------------------------------------------

### 7 Different treatments

* Fertilizer solution in closed bottles 
* Fertilizer solution in open bottles ($\mathrm{CO_2}$ from air)
* Fertilizer + Sugar (organic C source)
* Fertilizer + additional $\mathrm{HCO_3^-}$ (add $\mathrm{CaCO_3}$ to sparkling mineral water)
* A standard algae growth medium ("Basal medium") for comparison
* Deionized ("destilled") water and tap water for comparison

Experimental design
--------------------------------------------------------------------------------

<div align="center">
<img src="img/ruettler.jpg" height=600>
</div>

<div style="padding-left: 10ex;">
* each treatment with 3 replicates
* randomized experiment on a shaker
* 16:8 light:dark-cycle
* Measurement directly in the bottles using a self-made [turbidity meter](https://tpetzoldt.github.io/growthlab/doc/versuchsaufbau.html)
</div>

Results
--------------------------------------------------------------------------------

<div class="vbox"></div>
<div class="hbox">
![](img/ansaetze2.jpg)
Fertilizer -- Open Bottle -- F. + Sugar -- F. + CaCO3 -- Basal medium -- A. dest -- Tap water
</div>

The data set
--------------------------------

```{r, echo=FALSE}
dat <- data.frame(
  treat  = factor(c("Fertilizer", "Fertilizer", "Fertilizer", 
             "F. open", "F. open", "F. open", 
             "F.+sugar", "F.+sugar", "F.+sugar", 
             "F.+CaCO3", "F.+CaCO3", "F.+CaCO3", 
             "Bas.med.", "Bas.med.", "Bas.med.", 
             "A.dest", "A.dest", "A.dest", 
             "Tap water", "Tap water"),
             levels=c("Fertilizer", "F. open", "F.+sugar", 
                    "F.+CaCO3", "Bas.med.", "A.dest", "Tap water")),
  rep   = c(1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2), 
  growth = c(0.02, -0.217, -0.273, 0.94, 0.78, 0.555, 0.188, -0.1, 0.02, 
             0.245, 0.236, 0.456, 0.699, 0.727, 0.656, -0.01, 0, -0.01, 0.03, -0.07)
)

xdat <- 
  dat %>% 
  pivot_wider(id_cols = treat, names_from=rep, values_from=growth, names_prefix="replicate ")
```


```{r}
xdat %>% 
  kable('html', caption="Data set: Growth from day 2 to day 6 (in relative units)") %>% 
  kable_styling(bootstrap_options = "striped", full_width = FALSE)
```

* NA means "not available", i.e. a missing value
* The crosstable structure is compact and nice for a slide, but not suitable for
data analysis
* therefore, we use the long table format instead


Data in long format
--------------------------------------------------------------------------------

<div class='left' style='float:left;width:48%'>

<p style="padding-bottom: 30ex">&nbsp;</p>

## Advantages
* looks "stupid" but is better for data analysis
* dependend **growth** and explanation variable **treat** clearly visible
* easily extensible to $>1$ explanation variable
</div>

<div class='right' style='float:left;width:48%'>

```{r}
dat %>% 
  kable('html', caption="Data set: Growth from day 2 to day 6 (in relative units)") %>% 
  kable_styling(bootstrap_options = c("striped"), full_width = FALSE)
```

</div>

The data in R
--------------------------------------------------------------------------------

```{r, echo=TRUE}
dat <- data.frame(
  treat  = factor(c("Fertilizer", "Fertilizer", "Fertilizer", 
             "F. open", "F. open", "F. open", 
             "F.+sugar", "F.+sugar", "F.+sugar", 
             "F.+CaCO3", "F.+CaCO3", "F.+CaCO3", 
             "Bas.med.", "Bas.med.", "Bas.med.", 
             "A.dest", "A.dest", "A.dest", 
             "Tap water", "Tap water"),
             levels=c("Fertilizer", "F. open", "F.+sugar", 
                    "F.+CaCO3", "Bas.med.", "A.dest", "Tap water")),
  rep   = c(1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2), 
  growth = c(0.02, -0.217, -0.273, 0.94, 0.78, 0.555, 0.188, -0.1, 0.02, 
             0.245, 0.236, 0.456, 0.699, 0.727, 0.656, -0.01, 0, -0.01, 0.03, -0.07)
)
```

... can be read from a **csv**-file or entered directly in the code.

Visualization
--------------------------------------------------------------------------------

```{r echo=TRUE}
boxplot(growth ~ treat, data=dat)
abline(h=0, lty="dashed", col="grey")
```

But as we have only 2-3 replicates per box, it is better to plot the all values
separately using `stripchart`:

```{r}
stripchart(growth ~ treat, data=dat, vertical=TRUE)
```


Statistical approach and the Bonferroni law
--------------------------------------------------------------------------------

### Questions

* Are the treatments different?
* Which medium is the best?
* Is the best medium significantly better than the others?

### Hypotheses

* $H_0$ growth is the same in all treatments
* $H_A$ differences between media

### Why can't we apply just several t-tests?

* If we have 7 treatments and want to test all against each other, we would need $7 \cdot (7 - 1) / 2 = 21$&nbsp; tests.
* If we set $\alpha = 0.05$ we will get 5% false positives, i.e. one of 20 tests 
is on average false positive
* This means that we do N tests, we may increase the overall $\alpha$ error in the worst case to a value of $N\alpha$.
* This is called **alpha-error-inflation** or the **Bonferroni** law:

\[
\alpha_{total} \le \sum_{i=1}^{N} \alpha_i = N \cdot \alpha
\]


* If we ignore the Bonferroni law, we end in **statistical fishing** i.e. we
get spurious results just by chance.

### Solutions

* One approach can be to down-correct the alpha errors so that $\alpha_{total} = 0.05$
* The preferred approach is to use a method that does all tests simultanaeously: the ANOVA.

ANOVA: Analysis of variances
--------------------------------------------------------------------------------

### Basic Idea

* split the total variance into effect(s) and errors:

\[
s_y^2 = s^2_\mathrm{effect} + s^2_{\varepsilon}
\]

* The most surprising is, that we use variances to compare mean values. The reason 
for this is, that differences of means contribute to the total variance of the
whole sample. Sometimes, the variance components are also called **variance within** ($s^2_\varepsilon$) and variance **between** samples.
* The way how to separate variances is a linear model.

<p style="padding-bottom: 5ex">&nbsp;</p>

### Example

We have two brands of Clementines from a shop "E", that we encode as "EB" and "EP".
We want to know whether the premium brand ("P") and the basic brand ("B") have
a different weight.

Instead of a t-test we encode "EB" with 1 and "EP" with 2.  

```{r, echo=TRUE}
clem_edeka <- data.frame(
  brand = c("EP", "EB", "EB", "EB", "EB", "EB", "EB", "EB", "EB", "EB", "EB", 
            "EB", "EB", "EB", "EP", "EP", "EP", "EP", "EP", "EP", "EP", "EB", "EP"),
  weight = c(88, 96, 100, 96, 90, 100, 92, 92, 102, 99, 86, 89, 99, 89, 75, 80, 
             81, 96, 82, 98, 80, 107, 88)
)

clem_edeka$code <- as.numeric(factor(clem_edeka$brand))

plot(weight ~ code, data=clem_edeka, axe=FALSE)
m <- lm(weight ~ code, data=clem_edeka)
axis(1, at=c(1,2), labels=c("EB", "EP")); axis(2); box()
abline(m, col="blue")
```

### Total variance

```{r, echo=TRUE}
(var_tot <- var(clem_edeka$weight))
```

### Residual variance (alias "within variance")

```{r, echo=TRUE}
(var_res <- var(residuals(m)))
```

### Between variance or explained variance

```{r, echo=TRUE}
1 - var_res / var_tot
```


**Exercise:**

* Perform a t-Test for the two Clementine brands
* Compare the p-value of the t-test with the p-value of an ANOVA


ANOVA in R
--------------------------------------------------------------------------------

Back to the algae growth data. Let's call the linear model `m`:

```{r echo=TRUE}
m <- lm(growth ~ treat, data=dat)
```

We can then print the coefficients of the linear model with `summary(m)`, but 
the more common way is to use the `anova`function

```{r echo=TRUE}
anova(m)
```

The ANOVA table shows the F-tests testing for significance of all factors. In the table above, 
we have only one single factor.

We see that the treatment had a significant effect.


Posthoc tests
--------------------------------------------------------------------------------

The test above showed only, that the **factor** "treatment" had a significant effect, but we don't know which **levels** of the factor are different. Here we apply a so-called posthoc test.

Different posthoc tests exist, here we use the **Tukey HSD test** that is the most common.

The `TukeyHSD` function has a numerical and a graphical output.

### Tukey HSD test

```{r, echo=TRUE}
tk <- TukeyHSD(aov(m))
tk
```

### Graphical output

```{r, echo=TRUE}
par(las = 1)             # las = 1 make y annotation horizontal
par(mar = c(4, 10, 3, 1)) # more space at the left for axis annotation
plot(tk)
```



ANOVA assumptions and diagnostics
--------------------------------------------------------------------------------

The assumptions of the ANOVA are the same as for the linear model. In short:

1. Independence of errors
2. Variance homogeneity
3. Approximate normality of errors

Again, graphical methods are preferred. The easiest is `plot(m)`.

```{r echo=TRUE}
plot(m, which=1)
```


```{r echo=TRUE}
plot(m, which=2)
```

It is also possible to test variance homogeneity. Instead of an F-test that can only
compare two variances, we need a test that can compare more than two, for example the
Fligner-Killeen-test:

```{r, echo=TRUE}
fligner.test(growth ~ treat, data=dat)
```

One-way ANOVA with heterogeneous variances
--------------------------------------------------------------------------------

If variances are not equal we can use an extension of the Welch test for $\ge 2$ samples, in R called `oneway.test` instead of the one-way ANOVA :

```{r, echo=TRUE}
oneway.test(growth ~ treat, data=dat)
```


Two-way ANOVA
--------------------------------------------------------------------------------

```{r}
hams <- data.frame(No=1:12,
                   growth=c(6.6, 7.2, 6.9, 8.3, 7.9, 9.2,
                            8.3, 8.7, 8.1, 8.5, 9.1, 9.0),
                   diet= rep(c("A", "B", "C"), each=2),
                   coat= rep(c("light", "dark"), each=6)
                   )
```


* Example from a statistics text book [@Crawley2002]
* Effects of diet and coat color on growth of Hamsters  in Gramm per time (constructed data set)

<div style="align: center">
<img src="img/hams-crosstable.jpg" alt="Hamster growth" style="width:30%">
</div>

* Factorial experiment (**with replicates**)
* Each factor combination (cell) contains more than one observation.

Without replication: only one experiment per factor combination. This is possible,
but does not allow to identify interaction effects.

Tidy data
--------------------------------------------------------------------------------

```{r, echo=TRUE}
hams <- data.frame(No = 1:12,
                   growth = c(6.6, 7.2, 6.9, 8.3, 7.9, 9.2,
                            8.3, 8.7, 8.1, 8.5, 9.1, 9.0),
                   diet = rep(c("A", "B", "C"), each=2),
                   coat = rep(c("light", "dark"), each=6)
                   )
```

```{r}
hams %>% 
  kable('html', caption="Data set: Growth of hamsters (in gramm)") %>% 
  kable_styling(bootstrap_options = c("striped"), full_width = FALSE)
```

Visualization and ANOVA
--------------------------------------------------------------------------------
```{r}
par(mfrow=c(1, 2))
boxplot(growth ~ diet, data = hams, col="wheat")
boxplot(growth ~ coat, data=hams, col="wheat")
```

### ANOVA

```{r, echo=TRUE}
m <- lm(growth~coat*diet, data=hams)
anova(m)
```

### Interaction plot

```{r, echo=TRUE}
with(hams, interaction.plot(diet, coat, growth, col=c("brown", "orange"), lty=1, lwd=2))
```

Diagnostics
--------------------------------------------------------------------------------

### Assumptions

1. independence of measurements (within samples)
2. Variance homogeneity of residuals
3. Normal distribution of residuals

Note: test of assumptions only possible after fitting the model.

⇒ Fit the ANOVA model first, then check if it was correct!

### Diagnostic tools

* Box plot
* Plot of residuals vs. mean values
* Q-Q-plot of residuals
* Fligner-Killeen test (alternative: some people recommend the Levene-Test)

```{r, echo=TRUE}
par(mfrow=c(1, 2))
par(cex=1.2, las=1)
qqnorm(residuals(m))
qqline(residuals(m))

plot(residuals(m)~fitted(m))
abline(h=0)
```  

```{r, echo=TRUE}  
fligner.test(growth ~ interaction(coat, diet), data=hams)
```

Sequential Holm-Bonferroni method
--------------------------------------------------------------------------------

* Also called Holm procedure [@Holm1979]
* Easy to use
* Can be applied to any multiple test problem
* Less conservative that ordinary Bonferroni correction, but ...
* ... still a very conservative approach
* see also [Wikipedia](https://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method)

### Algorithm

1. Select smallest $p$ out of all $n$ $p$-values
2. If $p \cdot n < \alpha$ $\Rightarrow$ significant, else <bf>STOP</bf>
3. Set $n − 1 \rightarrow n$, remove smallest $p$ from the list and go to step 1.


Example
--------------------------------------------------------------------------------

Growth rate per day ($d^{-1}$) of blue-green algae cultures (*Pseudanabaena*) after adding
toxic peptides from another blue-green algae (*Microcystis*).

The original hypothesis was that Microcystin LR (MCYST) or a derivative of it 
(Substance A) inhibits growth.

```{r, echo=TRUE}
mcyst <-  data.frame(treat = factor(c(rep("Control", 5),
                                       rep("MCYST", 5),
                                       rep("Subst A", 5)),
                                levels=c("Control", "MCYST", "Subst A")),
                      mu   = c(0.086, 0.101, 0.086, 0.086, 0.099,
                               0.092, 0.088, 0.093, 0.088, 0.086,
                               0.095, 0.102, 0.106, 0.106, 0.106)
                     )
```

### Approach 1: one-way ANOVA

```{r, echo=TRUE}
par(mar=c(4, 8, 2, 1), las=1)
m <- lm(mu ~ treat, data=mcyst)
anova(m)
plot(TukeyHSD(aov(m)))
```

### Approach 2: multiple t-Tests with sequential Bonferroni correction

We separate the data set in single subsets:

```{r, echo=TRUE}
Control <- mcyst$mu[mcyst$treat == "Control"]
MCYST   <- mcyst$mu[mcyst$treat == "MCYST"]
SubstA  <- mcyst$mu[mcyst$treat == "Subst A"]
```

and perform 3 t-Tests:

```{r, echo=TRUE}
p1 <- t.test(Control, MCYST)$p.value
p2 <- t.test(Control, SubstA)$p.value
p3 <- t.test(MCYST, SubstA)$p.value
```

The following shows the raw p-values without correction:

```{r, echo=TRUE}
c(p1, p2, p3)
```

and with Holm correction:

```{r, echo=TRUE}
p.adjust(c(p1, p2, p3))
```
Conclusions
--------------------------------------------------------------------------------

### Statistical methods

* In case of Holm-corrected t-tests, only a signle p-value (MCYST vs. Subst A) remains significant. 
This indicates that in this case, Holm's method is more conservative than TukeyHSD 
(only one compared to two significant) effects.
* An ANOVA with posthoc test is in general preferred,
* but the sequential Holm-Bonferroni can be helpful in special cases. 
* Moreover, it demonstrates clearly that massive
multiple testing needs to be avoided.

$\Rightarrow$  ANOVA is to be preferred, when possible.

### Interpretation

* Regarding our original hypothesis, we can see that MCYST and SubstA did not
inhibit growth of *Pseudanabaena*. In fact SubstA stimulated growth.
* This was contrary to our expectations -- the biological reason was then found 10 years later.

More about this can be found in @Jahnichen2001, @Jahnichen2007, @Jahnichen2011, @Zilliges2011 or @Dziallas2011


ANCOVA
--------------------------------------------------------------------------------


```{r, echo=FALSE}
## Birth Weight Data see stats/demo/lm.glm.R
dobson <- data.frame(
  week = c(40, 38, 40, 35, 36, 37, 41, 40, 37, 38, 40, 38,
	 40, 36, 40, 38, 42, 39, 40, 37, 36, 38, 39, 40),
  weight = c(2968, 2795, 3163, 2925, 2625, 2847, 3292, 3473, 2628, 3176,
	    3421, 2975, 3317, 2729, 2935, 2754, 3210, 2817, 3126, 2539,
	    2412, 2991, 2875, 3231),
  sex = gl(2, 12, labels=c("M","F"))
)
plot(weight ~ week, data=dobson, col=c("blue", "red")[as.numeric(sex)], pch=16)
fem <- lm(weight ~ week, data=dobson, subset = sex=="F")
mal <- lm(weight ~ week, data=dobson, subset = sex=="M")
abline(fem, col="red")
abline(mal, col="blue")
```
Annette Dobson's birthweight data. A data set from a statistics textbook 
[@Dobson2013], birth weight of boys and girls in dependence of the
pregnancy week.

<hr></hr>

The data set is found at different places on the internet and in different versions.

Here the version that is found in an R demo: `demo(lm.glm)`

```{r echo=TRUE, eval=FALSE}
## Birth Weight Data see stats/demo/lm.glm.R
dobson <- data.frame(
  week = c(40, 38, 40, 35, 36, 37, 41, 40, 37, 38, 40, 38,
	 40, 36, 40, 38, 42, 39, 40, 37, 36, 38, 39, 40),
  weight = c(2968, 2795, 3163, 2925, 2625, 2847, 3292, 3473, 2628, 3176,
	    3421, 2975, 3317, 2729, 2935, 2754, 3210, 2817, 3126, 2539,
	    2412, 2991, 2875, 3231),
  sex = gl(2, 12, labels=c("M","F"))
)
```


Linear regression, ANCOVA and ANCOVA
--------------------------------------------------------------------------------

* ANCOVA (analysis of covariance) deals with the comparison of regression lines
* Simply speaking, we can distinguish the following:
    * independent variables have metric scale: linear regression
    * independent variables all nominal (factor): ANOVA
    * independent variables are mixed nominal and metric: ANCOVA
    
For the linear models discussed so far, the **dependent** variable is always 
metric, while binary or nominal dependent variables can be handled with generalized
linear models (GLM).


Anette Dobson's birthweight data
--------------------------------------------------------------------------------

Why not just using a t-test?

```{r, echo=TRUE}
boxplot(weight ~ sex,data=dobson, ylab="weight")
t.test(weight ~ sex, data=dobson, var.equal=TRUE)
```

The box plot shows much overlap and the difference is not significant, because
the t-test ignores important information: the pregnancy week.


ANCOVA makes use of covariates
--------------------------------------------------------------------------------



```{r, echo=TRUE}
m <- lm(weight ~ week * sex, data=dobson)
anova(m)
```

```{r, echo=FALSE}
plot(weight ~ week, data=dobson, col=c("blue","red")[as.numeric(sex)], pch=16)
p <- coef(m)
abline(a=p[1], b=p[2], col="red")
abline(a=p[1]+p[3], b=p[2]+p[4], col="blue")

fem <- lm(weight ~ week, data=dobson, subset = sex=="F")
mal <- lm(weight ~ week, data=dobson, subset = sex=="M")
abline(fem, col="black", lty="dashed")
abline(mal, col="black", lty="dashed")
```


### How this works

```{r, echo=TRUE}
plot(weight ~ week, data=dobson, col=c("blue","red")[as.numeric(sex)], pch=16)

summary(m)
p <- coef(m)
abline(a=p[1],      b=p[2],      col="red")
abline(a=p[1]+p[3], b=p[2]+p[4], col="blue")

## the result is the same as when we would fit separate linear models
fem <- lm(weight ~ week, data=dobson, subset = sex=="F")
mal <- lm(weight ~ week, data=dobson, subset = sex=="M")
abline(fem, col="black", lty="dashed")
abline(mal, col="black", lty="dashed")
```

Pitfalls of the ANOVA described so far
--------------------------------------------------------------------------------

1. Heterogeneity of variance
    * p-values can be biased (i.e. misleading or wrong)
    * use of a one-way ANOVA for uneaqual variances (Welch, 1951); in R: `oneway.test` 
2. Unbalanced case:<br>
  unequal number of samples for each factor combination
    * ANOVA results depend on the order of factors in the model formula.
    * Classical method: Type II or Type III ANOVA
    * Modern approach: model selection and likelihood ratio tests


Type II and Type III ANOVA
--------------------------------------------------------------------------------

* function `Anova` (with upper case `A`) in package **car**
* Help of function Anova:
"Type-II tests are calculated according to the principle of
marginality, testing each term after all others, except ignoring
the term’s higher-order relatives; so-called type-III tests violate
marginality, testing each term in the model after all of the others."
* Conclusion:
use Type II and don’t try to interpret single terms in case of significant 
interactions.

Type II and Type III ANOVA: Example
--------------------------------------------------------------------------------

```{r, echo=FALSE, fig.height=3}
par(mar=c(4.1, 5.1, 1.1, 1.1))
par(cex=1.2, las=1)
par(mfrow=c(1,2))
boxplot(growth~diet, data=hams, col="wheat", xlab="diet", ylab="growth")
boxplot(growth~coat, data=hams, col="wheat", xlab="coat")
```


```{r, echo=TRUE}
library(car, quiet=TRUE)
m <- lm(growth ~ coat * diet, data = hams)
Anova(m, type="II")
```


Model selection -- a paradigm change
--------------------------------------------------------------------------------

### Problem:

* In complicated models, p-values depend on number (and sometimes
  of order) of included factors and interactions.
* The $H_0$-based approach becomes confusing, e.g. because of
  contradictory p-values.

### Alternative approach:

Comparison of different model candidates instead of p-value based testing.

* Model with all potentiall effects → <span style="color:blue">full model</span>,
* Omit single factors → <span style="color:blue">reduced models</span> (several!),
* No influence factors (ony mean value) → <span style="color:blue">null model</span>.
* Which model is the best → <span style="color:blue">minimal adequate model</span>?

How can we measure which model is the best?
--------------------------------------------------------------------------------

Compromize between model fit and model complexity (number of
parameters, k).

* Goodness of fit: Likelihood L (measures how good the data match a
given model).
* Log Likelihood: makes the criterion additive.
* AIC (Akaike Information Criterion):

$$AIC = −2 \ln(L) + 2k$$

* Alternative: BIC (Bayesian Information Criterion), takes sample size
into account ($n$):

$$BIC = −2 \ln(L) + k · \ln(n)$$

The model with the smallest AIC (or BIC) is considered as 
<span style="color:blue">minimal adequate</span> (i.e. optimal) model.

Model Selection and Likelihood Ratio Tests
--------------------------------------------------------------------------------

```{r, echo=TRUE}
m1 <- lm(growth ~ diet * coat, data=hams)
m2 <- lm(growth ~ diet + coat, data=hams)
anova(m1, m2)
```

```{r}
AIC(m1, m2)
```

* Likelihood ratio test compares two models (`anova` with > 1 model)
* Model with interaction (`m1`) not significantly better than model without
interaction (`m2`).

**Conclusion:** take the simpler model

Automatic Model Selection
--------------------------------------------------------------------------------

* The full model is supplied to the `step` function.
* The modell with the smallest AIC ist the <span style="color:blue">minimal adequate</span> model:

```{r, echo=TRUE}
m1 <- lm(growth ~ diet * coat, data=hams)
step(m1)
```

Summary
--------------------------------------------------------------------------------
* Linear models form the basis of many statistical methods
    * Linear regression
    * ANOVA, ANCOVA, GLM, GAM, GLMM, . . .
    * ANOVA/ANCOVA instead of multiple testing
* ANOVA is more powerful than multiple tests: 
    * one big experiment needs less n than many small experiments together
    * identification of interaction effects
    * elimination of co-variates
* Model selection vs. p-value based testing
    * paradigm shift in statistics: AIC instead of p-value
    * more reliable, especially for imbalanced or complex designs
    * but: p-value based tests are sometimes easier to understand

Avoid p-value hacking
--------------------------------------------------------------------------------

### Do NOT repeat experiments until a significant p-value is found.

"**As debate rumbles on about how and how much poor statistics is to
blame for poor reproducibility, Nature asked influential statisticians to
recommend one change to improve science. The common theme? The
problem is not our maths, but ourselves.**"

<hr></hr>

### Five ways to fix statistics. Comment on Nature. 

Leek et al. (2017) https://doi.org/10.1038/d41586-017-07522-z

1. Jeff Leek: Adjust for human cognition
2. Blakeley B. McShane & Andrew Gelman: Abandon statistical significance
3. David Colquhoun: State false-positive risk, too
4. Michèle B. Nuijten: Share analysis plans and results
5. Steven N. Goodman: Change norms from within

<hr></hr>

Another blog post that aims to improve understanding:
http://daniellakens.blogspot.de/2017/12/understanding-common-misconceptions.html?m=1

<hr></hr>

**My conclusion:** The p-value is still useful but apply it with great care.


Copyright
--------------------------------------------------------------------------------

This resource was created by [tpetzoldt](github.com/tpetzoldt). It is provided 
as is without warranty.


Bibliography
--------------------------------------------------------------------------------
